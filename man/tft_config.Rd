% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/batch.R
\name{tft_config}
\alias{tft_config}
\title{Configuration for Tft models}
\usage{
tft_config(
  batch_size = 256,
  clip_value = NULL,
  loss = "quantile_loss",
  epochs = 5,
  drop_last = FALSE,
  total_time_steps = NULL,
  num_encoder_steps = NULL,
  quantiles = list(0.5),
  training_tau = 0.3,
  virtual_batch_size = 256,
  valid_split = 0,
  learn_rate = 0.02,
  optimizer = "adam",
  lr_scheduler = NULL,
  lr_decay = 0.1,
  step_size = 30,
  checkpoint_epochs = 10,
  cat_emb_dim = 1,
  hidden_layer_size = 160,
  dropout_rate = 0.3,
  stack_size = 3,
  num_heads = 1,
  verbose = FALSE,
  device = "auto"
)
}
\arguments{
\item{batch_size}{(int) Number of examples per batch, large batch sizes are
recommended. (default: 1024)}

\item{clip_value}{If a float is given this will clip the gradient at
clip_value. Pass \code{NULL} (default) to not clip.}

\item{loss}{(character or function) Loss function for training within
\link{"quantile_loss", "pinball_loss", "rmsse_loss", "smape_loss"}
(default to \link{"quantile_loss"})}

\item{epochs}{(int) Number of training epochs.}

\item{drop_last}{(bool) Whether to drop last batch if not complete during
training}

\item{total_time_steps}{(int) Size of the look-back time window + forecast horizon in steps.
This is the width of Temporal fusion decoder N.}

\item{num_encoder_steps}{(int) Size of the look-back time window in steps. This is the size
of LSTM encoder.}

\item{quantiles}{(list) list of quantiles forcasts to be used in quantile loss. (default = \link{list(0.5)}).}

\item{training_tau}{(float) training_tau value to be used in pinball loss. (default = 0.3).}

\item{virtual_batch_size}{(int) Size of the mini batches used for
Batch Normalization (default=256)}

\item{valid_split}{(float) The fraction of the dataset used for validation.}

\item{learn_rate}{initial learning rate for the optimizer.}

\item{optimizer}{the optimization method. currently only 'adam' is supported,
you can also pass any torch optimizer function.}

\item{lr_scheduler}{if \code{NULL}, (default) no learning rate decay is used. if \link{"step"}
decays the learning rate by \code{lr_decay} every \code{step_size} epochs. It can
also be a \link[torch:lr_scheduler]{torch::lr_scheduler} function that only takes the optimizer
as parameter. The \code{step} method is called once per epoch.}

\item{lr_decay}{multiplies the initial learning rate by \code{lr_decay} every
\code{step_size} epochs. Unused if \code{lr_scheduler} is a \code{torch::lr_scheduler}
or \code{NULL}.}

\item{step_size}{number of epoch before modifying learning rate by \code{lr_decay}.
Unused if \code{lr_scheduler} is a \code{torch::lr_scheduler} or \code{NULL}.}

\item{checkpoint_epochs}{checkpoint model weights and architecture every
\code{checkpoint_epochs}. (default is 10). This may cause large memory usage.
Use \code{0} to disable checkpoints.}

\item{cat_emb_dim}{(int or list) Embedding size for categorial features,
broadcasted to each categorical feature, or per categorical feature
when a list of the same size as the categorical features  (default=1)}

\item{hidden_layer_size}{(int)size of the Internal state layer (default=160).}

\item{dropout_rate}{dropout rate applied to each nn block (default=0.3)}

\item{stack_size}{(int) Number of self-attention layers to apply (default=3). Use 1 for
basic TFT.}

\item{num_heads}{(int) number of interpretable multi-attention head (default=1)}

\item{verbose}{(bool) wether to print progress and loss values during
training.}

\item{device}{the device to use for training. \link{"cpu"} or \link{"cuda"}. The default (\link{"auto"})
uses \link{"cuda"} if it's available, otherwise uses \link{"cpu"}.}
}
\value{
A named list with all hyperparameters of the TabNet implementation.
}
\description{
Configuration for Tft models
}
