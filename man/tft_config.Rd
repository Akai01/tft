% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/batch.R
\name{tft_config}
\alias{tft_config}
\title{Configuration for Tft models}
\usage{
tft_config(lookback, horizon, batch_size = 256, epochs = 5, verbose = FALSE)
}
\arguments{
\item{batch_size}{(int) Number of examples per batch, large batch sizes are
recommended. (default: 1024)}

\item{epochs}{(int) Number of training epochs.}

\item{verbose}{(bool) wether to print progress and loss values during
training.}

\item{total_time_steps}{(int) Size of the look-back time window + forecast horizon in steps.
This is the width of Temporal fusion decoder N.}

\item{num_encoder_steps}{(int) Size of the look-back time window in steps. This is the size
of LSTM encoder.}

\item{hidden_layer_size}{(int)size of the Internal state layer (default=160).}

\item{dropout_rate}{dropout rate applied to each nn block (default=0.3)}

\item{stack_size}{(int) Number of self-attention layers to apply (default=3). Use 1 for
basic TFT.}

\item{num_heads}{(int) number of interpretable multi-attention head (default=1)}

\item{loss}{(character or function) Loss function for training within
\code{"quantile_loss"}, \code{"pinball_loss"}, \code{"rmsse_loss"}, \code{"smape_loss"}
(default to \code{quantile_loss})}

\item{quantiles}{(list) list of quantiles forcasts to be used in quantile loss. (default = \code{list(0.5)}).}

\item{training_tau}{(float) training_tau value to be used in pinball loss. (default = 0.3).}

\item{clip_value}{If a float is given this will clip the gradient at
clip_value. Pass \code{NULL} (default) to not clip.}

\item{drop_last}{(bool) Whether to drop last batch if not complete during
training}

\item{virtual_batch_size}{(int) Size of the mini batches used for
Batch Normalization (default=256)}

\item{learn_rate}{initial learning rate for the optimizer.}

\item{optimizer}{the optimization method. currently only 'adam' is supported,
you can also pass any torch optimizer function.}

\item{valid_split}{(float) The fraction of the dataset used for validation.}

\item{lr_scheduler}{if \code{NULL}, (default) no learning rate decay is used. if \code{step}
decays the learning rate by \code{lr_decay} every \code{step_size} epochs. It can
also be a \code{torch::lr_scheduler} function that only takes the optimizer
as parameter. The \code{step} method is called once per epoch.}

\item{lr_decay}{multiplies the initial learning rate by \code{lr_decay} every
\code{step_size} epochs. Unused if \code{lr_scheduler} is a \code{torch::lr_scheduler}
or \code{NULL}.}

\item{step_size}{number of epoch before modifying learning rate by \code{lr_decay}.
Unused if \code{lr_scheduler} is a \code{torch::lr_scheduler} or \code{NULL}.}

\item{cat_emb_dim}{(int or list) Embedding size for categorial features,
broadcasted to each categorical feature, or per categorical feature
when a list of the same size as the categorical features  (default=1)}

\item{checkpoint_epochs}{checkpoint model weights and architecture every
\code{checkpoint_epochs}. (default is 10). This may cause large memory usage.
Use \code{0} to disable checkpoints.}

\item{device}{the device to use for training. \code{cpu} or \code{cuda}. The default (\code{auto})
uses \verb{cuda`` if it's available, otherwise uses }cpu`.}
}
\value{
A named list with all hyperparameters of the TabNet implementation.
}
\description{
Configuration for Tft models
}
