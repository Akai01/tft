% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit.R, R/parsnip.R
\name{tft}
\alias{tft}
\alias{tft_config}
\alias{temporal_fusion_transformer}
\title{Temporal Fusion Transformer}
\usage{
tft(x, ...)

tft_config(
  lookback,
  horizon,
  hidden_state_size = 16,
  num_attention_heads = 4,
  num_lstm_layers = 2,
  dropout = 0.1,
  batch_size = 256,
  epochs = 5,
  optimizer = "adam",
  learn_rate = 0.01,
  learn_rate_decay = c(0.1, 5),
  gradient_clip_norm = 0.1,
  quantiles = c(0.1, 0.5, 0.9),
  verbose = FALSE
)

temporal_fusion_transformer(
  mode = "regression",
  lookback = NULL,
  horizon = NULL,
  hidden_state_size = NULL,
  dropout = NULL,
  learn_rate = NULL,
  batch_size = NULL,
  epochs = NULL
)
}
\arguments{
\item{x}{A recipe containing \code{\link[=step_include_roles]{step_include_roles()}} as the last step. Can
also be a data.frame, but expect it to have a \code{recipe} attribute attribute
containing the \code{recipe} that generated it via \code{\link[recipes:bake]{recipes::bake()}} or
\code{\link[recipes:juice]{recipes::juice()}}.}

\item{...}{Additional arguments passed to \code{\link[=tft_config]{tft_config()}}.}

\item{lookback}{Number of timesteps that are used as historic data for
prediction.}

\item{horizon}{Number of timesteps ahead that will be predicted by the
model.}

\item{hidden_state_size}{Hidden size of network which is its main hyperparameter
and can range from 8 to 512. It's also known as \code{d_model} across the paper.}

\item{num_attention_heads}{Number of attention heads in the Multi-head attention layer.
The paper refer to it as \code{m_H}. \code{4} is a good default.}

\item{num_lstm_layers}{Number of LSTM layers used in the Locality Enhancement
Layer. Usually 2 is good enough.}

\item{dropout}{Dropout rate used in many places in the architecture.}

\item{batch_size}{How many samples per batch to load.}

\item{epochs}{Maximum number of epochs for training the model.}

\item{optimizer}{Optimizer used for training. Can be a string with 'adam', 'sgd',
or 'adagrad'. Can also be a \code{\link[torch:optimizer]{torch::optimizer()}}.}

\item{learn_rate}{Leaning rate used by the optimizer.}

\item{learn_rate_decay}{Decrease the learning rate by this factor each epoch.
Can also be a vector with 2 elements. In this case we decrease the learning by
the \code{x[1]} every \code{x[2]} epochs - (where \code{x} is the \code{learn_rate_decay} vector.)
Use \code{FALSE} or any negative number to disable.}

\item{gradient_clip_norm}{Maximum norm of the gradients. Passed on to
\code{\link[luz:luz_callback_gradient_clip]{luz::luz_callback_gradient_clip()}}. If <= 0 or \code{FALSE} then no gradient
clipping is performed.}

\item{quantiles}{A numeric vector with 3 quantiles for the quantile loss.
The first is treated as lower bound of the interval, the second as the
point prediction and the thir as the upper bound.}

\item{verbose}{Logical value stating if the model should produce status
outputs, like a progress bar, during training.}

\item{mode}{A single character string for the type of model.
The only possible value for this model is "regression".}
}
\value{
A list with the configuration parameters.
}
\description{
Temporal Fusion Transformer

Configuration for the Temporal Fusion Transformer network
}
\section{Functions}{
\itemize{
\item \code{tft_config}: Configuration configuration options for tft.

\item \code{temporal_fusion_transformer}: Parsnip wrappers for TFT.
}}

