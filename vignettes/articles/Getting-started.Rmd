---
title: "Getting started"
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
suppressPackageStartupMessages(library(tidymodels))
library(tft)
```

tft is an R implementation of Temporal Fusion Transformers (TFT) using the torch package. The Temporal Fusion Transformer is a neural network architecture proposed by Bryan Lim et al. with the goal of making interpretable multi-horizon time-series forecasts.

The R package tft abstracts away the details of the architecture and
provides an API that allows easy experimenting with the TFT architecture.

In this article we will create forecasts for the `?walmart_sales_weekly` dataset included in the [walmartdata](https://github.com/dfalbel/walmartdata) package. This dataset has weekly sales of a sample of
weekly sales by department of 45 retail stores. It also includes a few external predictors like the temperature,
fuel price and the size of the store.

```{r}
data(walmart_sales, package = "walmartdata")
dplyr::glimpse(walmart_sales)
```

To make the example faster to run, we are going to create models for the first 2
stores.

```{r}
walmart_sales <- walmart_sales %>% 
  filter(Store %in% c(1, 2, 3), Dept %in% c(1,2)) %>% 
  mutate(Store = as.factor(Store), Dept = as.factor(Dept))
```


## Preparing the data

The first thing we need to make sure is that our dataset doesn't have [implicit missing observations](https://r4ds.hadley.nz/missing-values.html#missing-values). This happens when an observations is just not present in
the data instead of being explicitly marked with a `NA`. We are going to use
`tsibble` functionality to add the implicitly missing observations, but you could
use whatever tool you prefer for the task.

```{r}
sales <- walmart_sales %>%
    tsibble::tsibble(
      key = c(Store, Dept, Type, Size),
      index = Date
    ) %>%
    tsibble::group_by_key() %>%
    tsibble::fill_gaps(
      Weekly_Sales = 0,
      IsHoliday = FALSE
    ) %>%
    tidyr::fill(Size, Temperature, Fuel_Price, CPI, Unemployment, .direction = "down")
```

tft can treat columns in the dataset differently depending on their types. There
are mainly 5 types of columns:

- **'index'**: is a single date column that specifies at which point in time 
  the observation refers to. This is not directly used by the model itself, but is
  used internally to create rolling windows and order observations.
- **'key'**: are groups of columns that identify a single time series. Keys
  are necessary if you are creating predictions for multiple time series in a 
  single model. By default, 'keys' are considered 'static' predictors by the model.
- **'static'**: predictors are considered 'static' when they don't vary over time,
  they are information from the time-series, like a region or a kind of product.
- **'unknown'** are predictors that vary over time but we only know values observed
  for past observations. For example, you can use the daily temperature as a predictor,
  but you only know it for past observations.
- **'known'** are predictors that vary over time and are known even for future observations.
  For example, the day of the week can be used as a predictor for a daily time series,
  and it's known for every time step, no matter if it's from past or future.

The `recipes` package is used to specify how the model should treat each column
of the dataset.

```{r}
rec <- recipe(Weekly_Sales ~ ., data = sales) %>% 
  step_mutate(
    time_since_begining = as.numeric(difftime(
      time1 = Date, 
      time2 = lubridate::ymd(min(sales$Date)), 
      units = "weeks"
    )),
    date_week = as.factor(lubridate::week(Date)),
    date_month = as.factor(lubridate::month(Date)),
    IsHoliday = as.factor(IsHoliday)
  ) %>% 
  step_impute_median(starts_with("MarkDown")) %>% 
  step_normalize(all_numeric_predictors())
```

It's recommended to include features that represent seasonality as known predictors
in the TFT model, like mon, day of the week and etc.
It's also recommended to normalize the predictors and treat missing values as
the model don't treat them implicitly. 

You can bake `prep` and `juice` the recipe to see how the transformations are working:

```{r}
rec %>% prep() %>% juice() %>% summary()
```

## Metrics and validation

Now we should think about the size of the horizon we want to create forecasts to.
It could be a single week ahead or ten, and this will influence how we split our
data for training, validation and testing. This is not really a data analysis
decision but more of a business decision, ie: how many weeks ahead we want know
so we can plan the demand and etc. Let's say we want 4 weeks ahead, ie ~1 month.

We are going to use the last 8 weeks for testing the model and the previous 8 weeks
for validation. All the other data will be used for training. Let's create the
datasets:

```{r}
last_date <- max(sales$Date)
sales_train <- sales %>% filter(Date <= (last_date - lubridate::weeks(24)))
sales_valid <- sales %>% filter(Date > (last_date - lubridate::weeks(24)),
                                Date <= (last_date - lubridate::weeks(8)))
sales_test <- sales %>% filter(Date > (last_date - lubridate::weeks(8)))
```

## Fitting the model

We will now fit our model. The `horizon` parameters states how many weeks ahead 
we want to generate predictions for. We discussed earlier that we want to predict 
4 weeks ahead. The lookback parameter defines how many weeks we are looking to make
predictions. We are going to use `100` in this example. Which is little more than 2
years.

```{r, eval = TRUE}
input_types <- covariates_spec(
  index = Date,
  keys = c(Store, Dept),
  static = c(Type, Size),
  known = c(starts_with("MarkDown"), time_since_begining, 
            starts_with("date_"), IsHoliday),
  unknown = c(Temperature, Fuel_Price)
)

model <- tft(
  rec, 
  sales_train,
  input_types = input_types,
  horizon = 4, 
  lookback = 54, 
  batch_size = 64,
  hidden_state_size = 128,
  verbose = TRUE,
  learn_rate = 0.001,
  epochs = 100,
  learn_rate_decay = FALSE,
  callbacks = list(
    luz::luz_callback_keep_best_model(monitor = "valid_loss"),
    luz::luz_callback_early_stopping(
      monitor = "valid_loss", 
      patience = 10, 
      min_delta = 0.001
    )
  ),
  valid_data = sales_valid
)
```

Once we are happy with the results of our model we can make predictions. 
The model we have trained can make predictions for 4 weeks ahead, but our validation
set has 8 weeks ahead. In order to validate the model, we are going to create 2 
validation sets and then combine the metrics:

```{r}
preds <- rolling_predict(model, past_data = sales_train, new_data = sales_valid)

p <- preds %>% 
  rowwise() %>% 
  summarise(
    past_data %>% 
      bind_rows(bind_cols(new_data, .pred)) %>% 
      mutate(.pred_at = max(past_data$Date))
  )

ggplot(p, aes(x = Date)) +
  geom_point(aes(y = Weekly_Sales)) +
  geom_line(aes(y = Weekly_Sales)) +
  geom_point(aes(y = .pred), color = "red") +
  geom_line(aes(y = .pred), color = "red") +
  facet_grid(Store + Dept ~ .pred_at, scales = "free_y")
```

```{r, eval = TRUE}
test_pred <- rolling_predict(
  model, 
  new_data = sales_test, 
  past_data = dplyr::bind_rows(tibble::as_tibble(sales_train), tibble::as_tibble(sales_valid))
)

p <- test_pred %>% 
  rowwise() %>% 
  summarise(
    past_data %>% 
      bind_rows(bind_cols(new_data, .pred)) %>% 
      mutate(.pred_at = max(past_data$Date))
  )

ggplot(p, aes(x = Date)) +
  geom_point(aes(y = Weekly_Sales)) +
  geom_line(aes(y = Weekly_Sales)) +
  geom_point(aes(y = .pred), color = "red") +
  geom_line(aes(y = .pred), color = "red") +
  facet_grid(Store + Dept ~ .pred_at, scales = "free_y")
```

To obtain predictions for future observations, first we need to load a 
data.frame that includes known predictors like the `IsHoliday` variable
for all time steps in the future data frame. Then we can call `forecast`
or `predict` to obtain the predictions for the future data.

Note that `forecast` or `predict` currently can only predict for 
`horizon` time steps ahead. And don't provide a away for doing 
rolling forecasts.

```{r, eval = TRUE}
new_data <- walmartdata::walmart_sales_test %>% 
  filter(Store %in% c(1, 2), Dept %in% c(1,2)) %>% 
  filter(Date <= lubridate::ymd("2012-11-23"))

final_model <- fit(model, sales)
final_predictions <- predict(final_model, new_data = new_data)
final_predictions
```
